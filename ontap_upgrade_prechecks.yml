---
- name: Run Through Pre-Checks Before Upgrading ONTAP   
  hosts: "{{ ansible_limit }}"
  gather_facts: no
  connection: 'local'
  collections:
    - netapp.ontap
  vars:
    failures: []
    warnings: []
    allowed_jobs: ["FabricPool Space Job", "SP Certificate Expiry Check"]
    cluster_rings: ["vldb","mgmt","vifmgr","bcomd","crs"]
    login: &login
      hostname: "{{ ontap_hostname }}"
      username: "{{ ontap_username }}"
      password: "{{ ontap_password }}"
      https: "{{ https }}"
      validate_certs: "{{ validate_certs }}"

  tasks:
    - block: 
      - name: Get cluster, node, aggr, vol, svm, network interfaces, port and SAN info
        include_tasks: tasks/ontap_get_cluster_info_rest.yml
        vars: 
          gather_subset: 
            - cluster
            - cluster/nodes
            - protocols/san/igroups
            - aggregate_info
            - volume_info
            - vserver_info
            - network/ip/interfaces
          parameters:
            fields: ['*']

      - name: Set various cluster info facts
        ansible.builtin.set_fact: 
          cluster_info: "{{ ontap_rest_info['cluster'] }}"
          cluster_node_count: "{{ ontap_rest_info['cluster/nodes']['num_records'] }}"
          cluster_nodes: "{{ ontap_rest_info['cluster/nodes']['records'] }}"
          san_igroups_count: "{{ ontap_rest_info['protocols/san/igroups']['num_records'] }}"
          aggregate_info: "{{ ontap_rest_info['storage/aggregates']['records'] }}"
          volume_info: "{{ ontap_rest_info['storage/volumes']['records'] }}"
          vserver_info: "{{ ontap_rest_info['svm/svms']['records'] }}"
          net_int: "{{ ontap_rest_info['network/ip/interfaces']['records'] }}"
      tags: 
        - onlyrunme
        
    - name: Check for bad disks
      include_tasks: tasks/ontap_get_cluster_info_rest.yml
      vars: 
        gather_subset: ['storage/disks'] #'protocols/san/igroups', 'cluster/nodes', 
        parameters:
          container_type: "broken|maintenance"
          state: "pending|reconstructing"
    
    - name: Log of bad disks check
            (netapp.ontap.na_ontap_rest_info, storage/disks, container_type=broken|maintenance, state=pending|reconstructing)
      ansible.builtin.debug:
        var: ontap_rest_info['storage/disks']

    - name: Fail playbook if any non-healthy disks
      include_tasks: tasks/build_summary.yml
      vars: 
        failure: ["There are bad disks.", "{{ ontap_rest_info['storage/disks']['records'] }}"]
      when: ontap_rest_info['storage/disks']['num_records'] > 0 
    
    - name: Check for any critical health alerts 
      netapp.ontap.na_ontap_rest_cli: 
        <<: *login
        command: 'system/health/alert'
        verb: "GET"
      register: verify_health_alerts
        
    - name: Log of verify_health_alerts 
            (health alert show)
      ansible.builtin.debug: 
        var: verify_health_alerts

    - name: Fail playbook if there are critical health alerts
      include_tasks: tasks/build_summary.yml
      vars: 
        failure: ["There are health alerts that need to 
          be addressed.", "{{ verify_health_alerts.msg.records }}"]
      when: verify_health_alerts.msg.num_records > 0

    - name: Check subsystem health
      netapp.ontap.na_ontap_rest_cli: 
        <<: *login
        command: 'system/health/subsystem'
        params: 
          fields: "health"
        verb: "GET"
      register: verify_health_subsystems

    - name: Log of verify_health_subsystems 
            (health subsystem show)
      ansible.builtin.debug: 
        var: verify_health_subsystems

    - name: Fail playbook if there are subsystems degraded 
      include_tasks: tasks/build_summary.yml
      vars: 
        failure: ["There are degraded subsystems that 
          need to be addressed.", "{{ verify_health_subsystems.msg.records }}"]
      when: verify_health_subsystems.msg.records|selectattr('health','!=','ok')|list|length > 0

    - name: Log of cluster node count and igroup count 
      ansible.builtin.debug:
        msg: "{{ item }}"
      loop:
      - "cluster_node_count = {{ cluster_node_count }}"
      - "san_igroups_count = {{ san_igroups_count }}"
    
    - name: CPU and Disk utilization should not exceed 50% 
        (statistics show-periodic -preset sysstat_utilization -iterations 10 -interval 3)
      netapp.ontap.na_ontap_ssh_command:
        <<: *login
        command: statistics show-periodic -preset sysstat_utilization -iterations 10 -interval 3
        privilege: adv
        accept_unknown_host_keys: true
      register: cpu_disk_check      
 
    - name: Log of cpu_disk_check
      ansible.builtin.debug: 
        var: cpu_disk_check['stdout']

    - name: Add warning cpu_disk_check having anything over 50% in the stdout
      include_tasks: tasks/build_summary.yml
      vars: 
        warning: ["There are CPU and disk utilizations over 50%.", 
                  "{{ cpu_disk_check['stdout'] }}"]
      when: cpu_disk_check | regex_findall('([5-9]\\d|[1-9]\\d{2,})(?=\\%)', multiline=True) | length > 0

    - name: Get epoch/quorum master info from the cluster ring 
      netapp.ontap.na_ontap_rest_cli: 
        <<: *login
        command: 'cluster/ring'
        params: 
          unitname: "{{ item }}"
          fields: "node,unitname,epoch,db-epoch,db-trnxs,master,online"
        verb: "GET"
      loop: "{{ cluster_rings }}"
      loop_control:
        label: "{{ item }}"
      register: verify_cluster_rings

    - name: Log of verify_cluster_rings
      ansible.builtin.debug: 
        var: verify_cluster_rings
      
    # to a.) verify the per-ring relational database epoch and database epoch match for 
            #   each node
            # b.) verify the per-ring quorum master is the same for all nodes. 
            #     [Each ring might have a different quorum master]
    - name: Fail playbook if  a.) the per-ring relational database epoch and database epoch do not match for 
            each node
            b.) the per-ring quorum master is not the same for all nodes. 
            (Each ring might have a different quorum master)
      include_tasks: tasks/build_summary.yml
      vars: 
        failure: ["The database epochs and/or quorum master do not match for each node.", "{{ item.msg.records }}"]
      loop: "{{ verify_cluster_rings.results }}"
      loop_control: 
        label: "{{ item.item }}"
      when: item.msg.records | map(attribute='epoch') | unique | length > 1 
            or item.msg.records | map(attribute='db_epoch') | unique | length > 1
            or item.msg.records | map(attribute='master') | unique | length > 1

    - block: 
      - name: > 
              For larger clusters, find epsilon
              (cluster show -fields epsilon)
        netapp.ontap.na_ontap_rest_cli: 
          <<: *login
          command: 'cluster'
          params: 
            fields: "epsilon"
          verb: "GET"
        register: cluster_epsilon

      - name: Log of cluster_epsilon
        ansible.builtin.debug: 
          var: cluster_epsilon

      - name: Set first_node_is_epsilon fact 
        ansible.builtin.set_fact:
          first_node_is_epsilon: "{{ cluster_epsilon.msg.records.0.epsilon }}"

      - name: >
              Set epsilon to false, if not on first node
              (cluster modify -node <epsilonnode> -epsilon false)     
        netapp.ontap.na_ontap_rest_cli: 
          <<: *login
          command: 'cluster'
          body: 
            epsilon: false 
          params: 
            node: "{{ (cluster_epsilon.msg.records |selectattr('epsilon','true')|list)[0]['node'] }}"  
          verb: "PATCH"
        register: move_cluster_epsilon
        when: first_node_is_epsilon == false

      - name: > 
              Set epsilon to true on first node
              (cluster modify -node <firstnode> -epsilon true)     
        netapp.ontap.na_ontap_rest_cli: 
          <<: *login
          command: 'cluster'
          body: 
            epsilon: true 
          params: 
            node: "{{ cluster_epsilon.msg.records.0.node }}"
          verb: "PATCH"
        register: move_cluster_epsilon
        when: first_node_is_epsilon == false

      - name: Log of move_cluster_epsilon
        ansible.builtin.debug: 
          var: move_cluster_epsilon
        when: first_node_is_epsilon == false

      - name: > 
              Confirm epsilon is at first node
              (cluster show -fields epsilon)
        netapp.ontap.na_ontap_rest_cli: 
          <<: *login
          command: 'cluster'
          params: 
            fields: "epsilon"
          verb: "GET"
        register: cluster_epsilon_confirm

      - name: Log of cluster_epsilon_confirm
        ansible.builtin.debug: 
          var: cluster_epsilon_confirm

      - name: Fail playbook if epsilon is not at first node
        include_tasks: tasks/build_summary.yml
        vars: 
          failure: ["`cluster show -fields epsilon` should have 
            {{ cluster_epsilon_confirm.msg.records.0.node }} as epsilon, 
            but currently 
            {{ (cluster_epsilon.msg.records |selectattr('epsilon','true')|list)[0]['node'] }}
            is set as epsilon"]
        when: cluster_epsilon_confirm.msg.records.0.epsilon != true

      when: cluster_node_count|int > 2
    
    - block: 

      - name: Verify each node is in SAN quorum
        netapp.ontap.na_ontap_rest_cli: 
          <<: *login
          command: 'event/log'
          params: 
            message-name: "scsiblade.*"
          verb: "GET"
        register: verify_san_quorum

      - name: Log of verify_san_quorum
        ansible.builtin.debug: 
          var: verify_san_quorum

      - name: Verify SAN (iscsi) configuration. Validate server side is set for redundancy.
        netapp.ontap.na_ontap_rest_cli: 
          <<: *login
          command: 'iscsi/initiator'
          params: 
            fields: "igroup,initiator-name,tpgroup"
          verb: "GET"
        register: verify_iscsi

      - name: Log of verify_iscsi
        ansible.builtin.debug: 
          var: verify_iscsi

      - name: Verify SAN (fcp) configuration. Validate server side is set for redundancy.
        netapp.ontap.na_ontap_rest_cli: 
          <<: *login
          command: 'fcp/initiator'
          params: 
            fields: "igroup,wwpn,lif"
          verb: "GET"
        register: verify_fcp

      - name: Log of verify_fcp
        ansible.builtin.debug: 
          var: verify_fcp

      - name: Verify SAN host configuration before proceeding
        include_tasks: tasks/build_summary.yml
        vars: 
          warning: ["Verify SAN configuration. Validate server side 
            is set for redundancy."]
      when: san_igroups_count | int > 0

    - name: For two node clusters, ensure HA is enabled 
      netapp.ontap.na_ontap_cluster_ha:
        <<: *login
        state: present
      when: cluster_node_count == 2 

    - name: Find any DP SnampMirrors
      netapp.ontap.na_ontap_rest_cli: 
        <<: *login
        command: 'snapmirror/list-destinations'
        params: 
          type: "DP"
        verb: "GET"
      register: verify_snapmirror_dp_destinations

    - name: Log of verify_snapmirror_dp_destinations
      ansible.builtin.debug: 
        var: verify_snapmirror_dp_destinations

    - name: Warn about DP SnapMirror upgrade order
      include_tasks: tasks/build_summary.yml
      vars: 
        warning: ["**Destination cluster must be upgraded prior 
          to upgrading source cluster for DP SnampMirrors**"]
      when: verify_snapmirror_dp_destinations.msg.num_records > 0

    - name: Get Snapshot copies 
      netapp.ontap.na_ontap_rest_cli: 
        <<: *login
        command: 'volume/snapshot'
        verb: "GET"
      register: verify_snapshot_count

    - name: Log of verify_snapshot_count
      ansible.builtin.debug: 
        var: verify_snapshot_count

    - name: Warn about DP SnapMirror upgrade order
      include_tasks: tasks/build_summary.yml
      vars: 
        failure: ["Each node must not exceed 20,000 snapshot copies"]
      when: verify_snapshot_count.msg.num_records > 20000

    # Haven't figured out the REST equivelant yet
    - name: > 
            List any deduplicated volumes over 96% 
            "volume show -is-sis-volume true -percent-used >96 -fields aggregate,percent-used"
      netapp.ontap.na_ontap_rest_cli: 
        <<: *login
        command: 'volume'
        params: 
          is-sis-volume: "true"
          percent-used: ">96"
          fields: "aggregate,percent-used"
        verb: "GET"
      register: verify_large_dedupe_volumes

    - name: Log of verify_large_dedupe_volumes
      ansible.builtin.debug: 
        var: verify_large_dedupe_volumes

    - block: 
      - name: If verify_large_dedupe_volumes isn't empty, gather list of aggrs involved 
        ansible.builtin.set_fact: 
          large_dedupe_volumes_aggrs: "{{ large_dedupe_volumes_aggrs|default([]) + [item.aggregate] }}"
        loop: "{{ verify_large_dedupe_volumes['msg']['records'] }}"

      - name: Log of large_dedupe_volumes_aggrs
        ansible.builtin.debug: 
          var: large_dedupe_volumes_aggrs

      - name: Check aggr containing deduplicated volumes
        netapp.ontap.na_ontap_ssh_command:
          <<: *login
          command: "df -A {{ large_dedupe_volumes_aggrs | join(',') }}"
          privilege: adv
          accept_unknown_host_keys: true
        register: verify_large_dedupe_volumes_aggrs

      - name: Log of verify_large_dedupe_volumes_aggrs
        ansible.builtin.debug: 
          var: verify_large_dedupe_volumes_aggrs

      - name: TODO - check / fail playbook if a row with over 97% is present
        include_tasks: tasks/build_summary.yml
        vars: 
          failure: ["Aggrs containing deduplicated volumes must not exceed 97% used capacity"]
        # TODO obvs 
        when: 1 == 1

      when: verify_large_dedupe_volumes.msg.num_records > 0

    - name: Verify NTP Servers
      ansible.builtin.debug: 
        var: cluster_info['ntp_servers']

    - name: Fail playbook if there are not at least 3 NTP Servers
      include_tasks: tasks/build_summary.yml
      vars: 
        failure: ["There must be at least three NTP servers per NetApp best practices"]
      when: cluster_info['ntp_servers']|length < 3

    - name: Verify cluster date 
      ansible.builtin.debug: 
        msg: "{{ item.date }}"
      loop: "{{ cluster_nodes }}"
      loop_control: 
        label: "{{ item.name }}"

    - name: Fail playbook if there is any variation among node timestamps 
      include_tasks: tasks/build_summary.yml
      vars: 
        failure: ["Dates and timezones must match on all nodes"]
      when: cluster_nodes | map(attribute='date') | list | unique | length > 1

    - name: Verify cluster health (cluster show)
      netapp.ontap.na_ontap_rest_cli: 
        <<: *login
        command: 'cluster'
        params: 
          fields: "health,eligibility"
        verb: "GET"
      register: verify_cluster_health

    - name: Log of verify_cluster_health
      ansible.builtin.debug: 
        var: verify_cluster_health

    - name: Fail playbook if eligibility or health is not true 
      include_tasks: tasks/build_summary.yml
      vars: 
        failure: ["Health and Eligibility must be `true` for all nodes"]
      loop: "{{ verify_cluster_health.msg.records }}"
      when: item.eligibility != true or item.health != true

    - name: Verify aggregates are all online
      ansible.builtin.debug: 
        msg: "{{ item.name }}: {{ item.state }}"
      loop: "{{ aggregate_info }}"
      loop_control:
        label: "{{ item.name }}"

    - name: Fail playbook for any offline aggrs 
      include_tasks: tasks/build_summary.yml
      vars: 
        failure: ["All aggregates must be online"]
      when: aggregate_info | selectattr('state','!=','online') | list | length >= 1

    - name: Verify volumes are all online
      ansible.builtin.debug: 
        msg: "{{ item.svm.name }}/{{ item.name }}: {{ item.state }}"
      loop: "{{ volume_info }}"
      loop_control:
        label: "{{ item.svm.name }}/{{ item.name }}"

    - name: Fail playbook for any offline volumes 
      include_tasks: tasks/build_summary.yml
      vars: 
        failure: ["All volumes must be online"]
      when: volume_info | selectattr('state','!=','online') | list | length >= 1

    - name: Verify svm's are all running
      ansible.builtin.debug: 
        msg: "{{ item.name }}: {{ item.state }}"
      loop: "{{ vserver_info }}"
      loop_control:
        label: "{{ item.name }}"

    - name: Fail playbook for any svm's not running
      include_tasks: tasks/build_summary.yml
      vars: 
        failure: ["All SVMs must be online"]
      when: vserver_info | selectattr('state','!=','running') | list | length >= 1

    # TODO - this feels janky and I think I should be able to do this without a loop
    - name: Check for any LIFs not being home
      ansible.builtin.set_fact: 
        net_int_not_home: "{{ net_int_not_home | default([]) + [item] }}"
      loop: "{{ net_int }}"
      loop_control:
        label: "{{ item.name }}"
      when: item.location.is_home == False

    - block:
      - name: Log of LIFs not home
        ansible.builtin.debug: 
          var: net_int_not_home

      - name: All LIFs should be homed prior to upgrades
        netapp.ontap.na_ontap_rest_cli:
          <<: *login
          command: "network/interface/revert"
          params: 
            "vserver": "*"
          verb: "PATCH"
        register: verify_net_int_revert

      - name: Log of verify_net_int_revert
        ansible.builtin.debug: 
          var: verify_net_int_revert
      
      - name: Check to make sure everything got home (net int show -is-home false)
        netapp.ontap.na_ontap_rest_cli: 
          <<: *login
          command: 'network/interface'
          params: 
            is-home: "false"
          verb: "GET"
        register: verify_all_lifs_home

      - name: Log of verify_all_lifs_home
        ansible.builtin.debug: 
          var: verify_all_lifs_home

      - name: Fail playbook if not all LIFs are home 
        include_tasks: tasks/build_summary.yml
        vars: 
          failure: ["All LIFs must be home", "{{ verify_all_lifs_home.msg.records }}"]
        when: verify_all_lifs_home.msg.num_records > 0

      when: net_int_not_home is defined

    - name: Check for any ethernet ports that are UP but aren't healthy    
      netapp.ontap.na_ontap_rest_cli: 
        <<: *login
        command: 'network/port'
        params: 
          link: "up"
          health-status: "!healthy"
        verb: "GET"
      register: verify_ports_unhealthy

    - name: Fail playbook for any unhealthy ports
      include_tasks: tasks/build_summary.yml
      vars: 
        failure: ["All link=up ports must be healthy", "{{ verify_ports_unhealthy.msg.records }}"]
      when: verify_ports_unhealthy.msg.num_records > 0

    - name: Get failover groups (`net int failover-groups show`)
      netapp.ontap.na_ontap_rest_cli: 
        <<: *login
        command: 'network/interface/failover-groups'
        params: 
          fields: "vserver,targets,failover-group"
        verb: "GET"
      register: verify_net_int_failover_groups
      tags:         
        - onlyrunme
    
    - name: Log of verify_net_int_failover_groups
      ansible.builtin.debug: 
        var: verify_net_int_failover_groups
      tags:         
        - onlyrunme

    - name: Fail playbook if, for each failover group, a) There isn't at least 1 port from each node 
              b) The vlan tags do not match
      include_tasks: tasks/build_summary.yml
      args:
        apply:
          tags: onlyrunme
      vars: 
        failure: ["Each failover group must have at least 1 port from each node ({{ cluster_nodes | map(attribute='name') | join(',') }}) and the vlan tags must 
                    match", "{{ verify_net_int_failover_groups.msg.records | ontap_find_invalid_failover_groups(cluster_nodes) }}"]
      when: verify_net_int_failover_groups.msg.records | ontap_find_invalid_failover_groups(cluster_nodes) | length > 0
      tags:         
        - onlyrunme

    - name: Get broadcast domains (`broadcast-domain show`)  
      netapp.ontap.na_ontap_rest_cli: 
        <<: *login
        command: 'network/port/broadcast-domain'
        params: 
          fields: "broadcast-domain,ports,mtu,ipspace,failover-groups"
        verb: "GET"
      register: verify_broadcast_domains
      tags:         
        - onlyrunme

    - name: Log of verify_broadcast_domains
      ansible.builtin.debug: 
        var: verify_broadcast_domains
      tags:         
        - onlyrunme

    - name: Fail playbook if, for each broadcast domain, a) There isn't at least 1 port from each node 
              b) The vlan tags do not match
      include_tasks: tasks/build_summary.yml
      args:
        apply:
          tags: onlyrunme
      vars: 
        failure: ["Each broadcast domain must have at least 1 port from each node ({{ cluster_nodes | map(attribute='name') | join(',') }}) and the vlan tags must 
                    match", "{{ verify_broadcast_domains.msg.records | ontap_find_invalid_broadcast_domains(cluster_nodes) }}"]
      when: verify_broadcast_domains.msg.records | ontap_find_invalid_broadcast_domains(cluster_nodes) | length > 0
      tags:         
        - onlyrunme
    

    # TODO - finish failover group / broadcast domain 
  
    - name: Get running jobs 
      netapp.ontap.na_ontap_rest_cli: 
        <<: *login
        command: 'job'
        params: 
          fields: "name,id,node,state,vserver"
          state: "running"  
        verb: "GET"
      register: verify_running_jobs

    - name: Log of verify_running_jobs
      ansible.builtin.debug: 
        var: verify_running_jobs

    - name: Fail playbook if any jobs are running besides {{ allowed_jobs | join(',') }}
      include_tasks: tasks/build_summary.yml  
      vars: 
        failure: ["No jobs should be running", "{{ verify_running_jobs.msg.records }}"]
      when: verify_running_jobs.msg.num_records > 0 
              and 
            verify_running_jobs.msg.records | map(attribute='name') | difference(allowed_jobs) | length > 0

  # TODO - finish adding rest of checks
    - name: Show warnings, if any
      ansible.builtin.fail: 
        msg: "See Above Warnings"
      when: warnings | warning | length > 0 
      ignore_errors: True
      tags: 
        - onlyrunme

    - name: Fail playbook if any of the health checks didn't pass
      ansible.builtin.fail: 
        msg: "{{ failures }}"
      when: failures | length > 0
      tags: 
        - onlyrunme

# TODO - nicely summarize checks in one easy to digest snapshot


