---
- name: Run Through Health Checks Before/After Upgrading ONTAP   
  hosts: "{{ ansible_limit }}"
  gather_facts: no
  connection: 'local'
  collections:
    - netapp.ontap
  vars:
    summary_failures: []
    summary_warnings: []
    summary_logs: []
    allowed_jobs: ["FabricPool Space Job", "SP Certificate Expiry Check Job"]
    cluster_rings: ["vldb","mgmt","vifmgr","bcomd","crs"]
    login: &login
      hostname: "{{ ontap_hostname }}"
      username: "{{ ontap_username }}"
      password: "{{ ontap_password }}"
      https: "{{ https }}"
      validate_certs: "{{ validate_certs }}"

  tasks:
    - name: Get basic info needed and set variables (`netapp.ontap.na_ontap_rest_info`)
      block: 
      
      - name: Get cluster, node, aggr, vol, svm, network interfaces, port and SAN info
        include_tasks: tasks/ontap_get_cluster_info_rest.yml
        vars: 
          gather_subset: 
            - cluster
            - cluster/nodes
            - protocols/san/igroups
            - aggregate_info
            - volume_info
            - vserver_info
            - network/ip/interfaces
          parameters:
            fields: ['*']

      - name: Set needed variables
        ansible.builtin.set_fact: 
          cluster_info: "{{ ontap_rest_info['cluster'] }}"
          cluster_node_count: "{{ ontap_rest_info['cluster/nodes']['num_records'] }}"
          cluster_nodes: "{{ ontap_rest_info['cluster/nodes']['records'] }}"
          san_igroups: "{{ ontap_rest_info['protocols/san/igroups']['records'] }}"          
          san_igroups_count: "{{ ontap_rest_info['protocols/san/igroups']['num_records'] }}"
          aggregate_info: "{{ ontap_rest_info['storage/aggregates']['records'] }}"
          volume_info: "{{ ontap_rest_info['storage/volumes']['records'] }}"
          vserver_info: "{{ ontap_rest_info['svm/svms']['records'] }}"
          net_int: "{{ ontap_rest_info['network/ip/interfaces']['records'] }}"

      - name: Log of cluster node count and igroup count 
        include_tasks: tasks/add_log.yml
        vars: 
          log: 
            cluster_nodes: "{{ cluster_nodes | map(attribute='name') }}"
            cluster_node_count: "{{ cluster_node_count | int }}"
            san_igroups_count: "{{ san_igroups_count | int }}"
            san_igroups: "{{ san_igroups | map(attribute='name') }}"

      tags: ["always"]

    - name: Check overall health of the cluster
      block: 

      - name: Check for any critical health alerts (`health alert show`)
        netapp.ontap.na_ontap_rest_cli: 
          <<: *login
          command: 'system/health/alert'
          verb: "GET"
        register: verify_health_alerts
          
      - name: Fail playbook if there are critical health alerts
        include_tasks: tasks/add_failure.yml
        args:
          apply:
            tags: ["health"]
        vars: 
          failure: 
            issue: "There are critical health alerts" 
            details: "{{ verify_health_alerts.msg.records }}"
        when: verify_health_alerts.msg.num_records > 0

      - name: Check subsystem health (`health subsystem show`)
        netapp.ontap.na_ontap_rest_cli: 
          <<: *login
          command: 'system/health/subsystem'
          params: 
            fields: "health"
          verb: "GET"
        register: verify_health_subsystems

      - name: Log of verify_health_subsystems
        include_tasks: tasks/add_log.yml
        args:
          apply:
            tags: ["health"]
        vars: 
          log: 
            health_subsystems: "{{ verify_health_subsystems }}"

      - name: Fail playbook if there are subsystems degraded 
        include_tasks: tasks/add_failure.yml
        args:
          apply:
            tags: ["health"]
        vars: 
          failure: 
            issue: "There are subsystems that are not in a healthy state"
            details: "{{ verify_health_subsystems.msg.records }}"
        when: verify_health_subsystems.msg.records|selectattr('health','!=','ok')|list|length > 0

      - name: Verify cluster health (`cluster show`)
        netapp.ontap.na_ontap_rest_cli: 
          <<: *login
          command: 'cluster'
          params: 
            fields: "health,eligibility"
          verb: "GET"
        register: verify_cluster_health

      - name: Log of verify_cluster_health
        include_tasks: tasks/add_log.yml
        args:
          apply:
            tags: ["health"]
        vars: 
          log: 
            cluster_health: "{{ verify_cluster_health }}"

      - name: Fail playbook if eligibility or health is not true 
        include_tasks: tasks/add_failure.yml
        args:
          apply:
            tags: ["health"]
        vars: 
          failure: 
            issue: "Health and Eligibility must be `true` for all nodes"
            details: "{{ verify_cluster_health.msg.records }}"
        when: verify_cluster_health.msg.records | selectattr('eligibility','eq','false') | 
              selectattr('health','eq', 'false') | list | length > 0

      - name: Verify failover status (`storage failover show`)  
        netapp.ontap.na_ontap_rest_cli: 
          <<: *login
          command: 'storage/failover'
          params: 
            fields: "node,partner-name,enabled,possible,state-description"
          verb: "GET"
        register: verify_failover_status

      - name: Log of verify_failover_status
        include_tasks: tasks/add_log.yml
        args:
          apply:
            tags: ["health"]
        vars: 
          log: 
            failover_status: "{{ verify_failover_status }}"

      - name: Check for any nodes where failover isn't possible
        ansible.builtin.set_fact: 
          failover_not_possible: "{{ verify_failover_status.msg.records | selectattr('possible','false') | list }}"

      - name: Enable failover for any nodes where it's disabled on 4+ node clusters (later, we will ensure two node clusters are in HA mode, which automatically turns on storage failover)  
        block: 

        - name: Log of failover_not_possible
          include_tasks: tasks/add_log.yml
          args:
            apply:
              tags: ["health"]
          vars: 
            log: 
              failover_not_possible: "{{ failover_not_possible }}"

        - name: Enable storage failover
          netapp.ontap.na_ontap_storage_failover:
            <<: *login
            state: present
            node_name: "{{ failover_not_possible | map(attribute='node') | join(',') }}"
          register: enable_failover

        # - name: Enable failvoer 
        #   netapp.ontap.na_ontap_rest_cli: 
        #     <<: *login
        #     command: 'storage/failover'
        #     body: 
        #       enabled: true
        #     params: 
        #       node: "{{ failover_not_possible | map(attribute='node') | join(',') }}"
        #     verb: "PATCH"
        #   register: enable_failover

        - name: Log of enable_failover
          include_tasks: tasks/add_log.yml
          args:
            apply:
              tags: ["health"]
          vars: 
            log: 
              enable_failover: "{{ enable_failover }}"

        - name: Fail playbook if failover was not enabled on all nodes
          include_tasks: tasks/add_failure.yml
          args:
            apply:
              tags: health
          vars: 
            failure: 
              issue: "Failover was not sucessfully enabled on all nodes"
              details: "{{ failover_not_possible }}"
          when: enable_failover.failed == true or enable_failover.msg.num_records != failover_not_possible | length 

        when: failover_not_possible | length > 0 and cluster_node_count | int > 2

      tags: ["health"]

    - name: Check system utilization  
      block: 
    
      - name: CPU and Disk utilization should not exceed 50% 
          (statistics show-periodic -preset sysstat_utilization -iterations 10 -interval 3)
        netapp.ontap.na_ontap_ssh_command:
          <<: *login
          command: statistics show-periodic -preset sysstat_utilization -iterations 10 -interval 3
          privilege: adv
          accept_unknown_host_keys: true
        register: cpu_disk_check      
  
      - name: Log of cpu_disk_check
        include_tasks: tasks/add_log.yml
        args:
          apply:
            tags: ["cpu_disk"]
        vars: 
          log: 
            cpu_disk_check: "{{ cpu_disk_check.stdout }}"

      - name: Add warning cpu_disk_check having anything over 50% in the stdout
        include_tasks: tasks/add_warning.yml
        args:
          apply:
            tags: ["cpu_disk"]
        vars: 
          warning:
            issue: "There are CPU and disk utilizations over 50%."
            details: "{{ cpu_disk_check['stdout'] }}"
        when: cpu_disk_check | regex_findall('([5-9]\\d|[1-9]\\d{2,})(?=\\%)', multiline=True) | length > 0
      
      tags: ["cpu_disk"]

    - name: Check cluster ring database epoch and quorum master
      block: 

      - name: Get epoch/quorum master info from the cluster ring 
        netapp.ontap.na_ontap_rest_cli: 
          <<: *login
          command: 'cluster/ring'
          params: 
            unitname: "{{ item }}"
            fields: "node,unitname,epoch,db-epoch,db-trnxs,master,online"
          verb: "GET"
        loop: "{{ cluster_rings }}"
        loop_control:
          label: "{{ item }}"
        register: verify_cluster_rings

      - name: Log of verify_cluster_rings
        include_tasks: tasks/add_log.yml
        args: 
          apply:
            tags: ["cluster_rings"]
        vars: 
          log: 
            "verify_cluster_rings": "{{ item.msg.records }}"
        loop: "{{ verify_cluster_rings.results }}"
        loop_control:
          label: "{{ item.item }}"

        
      - name: Fail playbook if  a.) the per-ring relational database epoch and database epoch do not match for 
              each node
              b.) the per-ring quorum master is not the same for all nodes. 
              (Each ring might have a different quorum master)
        include_tasks: tasks/add_failure.yml
        args: 
          apply:
            tags: ["cluster_rings"]
        vars: 
          failure: 
            issue: "The database epochs and/or quorum master do not match for each node for {{ item.item }}" 
            details: "{{ item.msg.records }}"
        loop: "{{ verify_cluster_rings.results }}"
        loop_control: 
          label: "{{ item.item }}"
        when: item.msg.records | map(attribute='epoch') | unique | length > 1 
              or item.msg.records | map(attribute='db_epoch') | unique | length > 1
              or item.msg.records | map(attribute='master') | unique | length > 1

      tags: ["cluster_rings"]
    
    - name: Make sure cluster epsilon is at first node 
      block: 
      - name: For larger clusters, find epsilon (`cluster show -fields epsilon`)
        netapp.ontap.na_ontap_rest_cli: 
          <<: *login
          command: 'cluster'
          params: 
            fields: "epsilon"
          verb: "GET"
        register: cluster_epsilon

      - name: Log of cluster_epsilon
        include_tasks: tasks/add_log.yml
        args: 
          apply:
            tags: ["epsilon"]
        vars: 
          log: 
            cluster_epsilon: "{{ cluster_epsilon }}"

      - name: Set first_node and epsilon_node facts 
        ansible.builtin.set_fact:
          first_node: "{{ cluster_epsilon.msg.records | first }}"
          epsilon_node: "{{ cluster_epsilon.msg.records |selectattr('epsilon','true') | first | default(omit) }}"

      - name: Set epsilon to false, if not on first node (`cluster modify -node <epsilonnode> -epsilon false`)     
        netapp.ontap.na_ontap_rest_cli: 
          <<: *login
          command: 'cluster'
          body: 
            epsilon: false 
          params: 
            node: "{{ epsilon_node.node }}"  
          verb: "PATCH"
        register: move_cluster_epsilon
        # Catch edge case where epsilon is not true for any node
        when: first_node.epsilon == false and epsilon_node is defined 

      - name: Set epsilon to true on first node (`cluster modify -node <firstnode> -epsilon true`)     
        netapp.ontap.na_ontap_rest_cli: 
          <<: *login
          command: 'cluster'
          body: 
            epsilon: true 
          params: 
            node: "{{ first_node.node }}"
          verb: "PATCH"
        register: move_cluster_epsilon
        when: first_node.epsilon == false

      - name: Log of move_cluster_epsilon
        include_tasks: tasks/add_log.yml
        args: 
          apply:
            tags: ["epsilon"]
        vars: 
          log: 
            cluster_epsilon_move: "{{ move_cluster_epsilon }}"
        when: first_node.epsilon == false

      - name: Confirm epsilon is at first node (`cluster show -fields epsilon`)
        netapp.ontap.na_ontap_rest_cli: 
          <<: *login
          command: 'cluster'
          params: 
            fields: "epsilon"
          verb: "GET"
        register: cluster_epsilon_confirm

      - name: Log of cluster_epsilon_confirm
        include_tasks: tasks/add_log.yml
        args: 
          apply:
            tags: ["epsilon"]
        vars: 
          log: 
            cluster_epsilon_confirm: "{{ cluster_epsilon_confirm }}"

      - name: Fail playbook if epsilon is not at first node
        include_tasks: tasks/add_failure.yml
        args: 
          apply:
            tags: ["epsilon"]
        vars:
          failure: 
            issue: "{{ first_node.node }} should be epsilon, but currently {{ epsilon_node.node | default('no node') }} is set as epsilon"
            details: "{{ cluster_epsilon_confirm }}"
        when: cluster_epsilon_confirm.msg.records.0.epsilon == false
      
      when: cluster_node_count | int > 2
      tags: ["epsilon"]
    
    - name: Check SAN configuration, if applicable
      block: 

      - name: Verify each node is in SAN quorum (`event log show -message-name scsiblade.*`)
        netapp.ontap.na_ontap_rest_cli: 
          <<: *login
          command: 'event/log'
          params: 
            message-name: "scsiblade.*"
          verb: "GET"
        register: verify_san_quorum

      - name: Log of verify_san_quorum
        include_tasks: tasks/add_log.yml
        args: 
          apply:
            tags: ["san"]
        vars: 
          log: 
            san_quorum: "{{ verify_san_quorum }}"

      - name: Verify SAN (iscsi) configuration. Validate server side is set for redundancy. (`iscsi initiator show -fields igroup,initiator-name,tpgroup`)
        netapp.ontap.na_ontap_rest_cli: 
          <<: *login
          command: 'iscsi/initiator'
          params: 
            fields: "igroup,initiator-name,tpgroup"
          verb: "GET"
        register: verify_iscsi

      - name: Log of verify_iscsi
        include_tasks: tasks/add_log.yml
        args: 
          apply:
            tags: ["san"]
        vars: 
          log: 
            iscsi: "{{ verify_iscsi }}"

      - name: Verify SAN (fcp) configuration. Validate server side is set for redundancy. (`fcp initiator show -fields igroup,wwpn,lif`)
        netapp.ontap.na_ontap_rest_cli: 
          <<: *login
          command: 'vserver/fcp/initiator'
          params: 
            fields: "igroup,wwpn,lif"
          verb: "GET"
        register: verify_fcp

      - name: Log of verify_fcp
        include_tasks: tasks/add_log.yml
        args: 
          apply:
            tags: ["san"]
        vars: 
          log: 
            fcp: "{{ verify_fcp }}"

      - name: Verify SAN host configuration before proceeding
        include_tasks: tasks/add_warning.yml
        args:
          apply:
            tags: ["san"]
        vars: 
          warning: 
            issue: "Verify SAN configuration. Validate server side is set for redundancy."
            details: 
              iscsi: "{{ verify_iscsi.msg.records }}"
              fcp: "{{ verify_fcp.msg.records }}"
      
      when: san_igroups_count | int > 0
      tags: ["san"] 

    - name: Check SnapMirrors and SnapShot count
      block: 

      - name: Find any DP SnampMirrors (`snapmirror list-destinations`)
        netapp.ontap.na_ontap_rest_cli: 
          <<: *login
          command: 'snapmirror/list-destinations'
          params: 
            type: "DP"
          verb: "GET"
        register: verify_snapmirror_dp_destinations

      - name: Log of verify_snapmirror_dp_destinations
        include_tasks: tasks/add_log.yml
        args: 
          apply:
            tags: ["dp_snapmirror"]
        vars: 
          log: 
            snapmirror_dp: "{{ verify_snapmirror_dp_destinations }}"

      - name: Warn about DP SnapMirror upgrade order
        include_tasks: tasks/add_warning.yml
        args:
          apply:
            tags: ["dp_snapmirror"]
        vars: 
          warning: 
            issue: "**Destination cluster must be upgraded prior 
            to upgrading source cluster for DP SnampMirrors**"
            details: "{{ verify_snapmirror_dp_destinations }}"
        when: verify_snapmirror_dp_destinations.msg.num_records > 0

      tags: ["dp_snapmirror"]

    - name: Check snapshot count
      block: 

      - name: Get Snapshot copies (`vol snapshot show`)
        netapp.ontap.na_ontap_rest_cli: 
          <<: *login
          command: 'volume/snapshot'
          verb: "GET"
        register: verify_snapshot_count

      - name: Log of verify_snapshot_count
        include_tasks: tasks/add_log.yml
        args: 
          apply:
            tags: ["snapshots"]
        vars: 
          log: 
            snapshot_count: "{{ verify_snapshot_count.msg.num_records }}"

      - name: Fail playbook if there are more than 20,000 snapshot copies
        include_tasks: tasks/add_failure.yml
        args:
          apply:
            tags: ["snapshots"]
        vars: 
          failure: 
            issue: "Each node must not exceed 20,000 snapshot copies"
            details: "{{ verify_snapshot_count.msg.num_records }}"
        when: verify_snapshot_count.msg.num_records > 20000

      tags: ["snapshots"]

    - name: Check for bad disks
      block:

      - name: Get broken/maintenance disks
              (netapp.ontap.na_ontap_rest_info, storage/disks, container_type=broken|maintenance)
        include_tasks: tasks/ontap_get_cluster_info_rest.yml
        vars: 
          gather_subset: ['storage/disks']  
          parameters:
            container_type: "broken|maintenance"

      - name: Set broken_disks
        ansible.builtin.set_fact: 
          broken_disks: "{{ ontap_rest_info['storage/disks']['records'] }}"

      - name: Log of broken_disks
        include_tasks: tasks/add_log.yml
        args: 
          apply:
            tags: ["disks"]
        vars: 
          log: 
            broken_disks: "{{ broken_disks }}"

      - name: Get pending/reconstructing disks 
              (netapp.ontap.na_ontap_rest_info, storage/disks, state=pending|reconstruction)
        include_tasks: tasks/ontap_get_cluster_info_rest.yml
        vars: 
          gather_subset: ['storage/disks']  
          parameters:
            state: "pending|reconstruction"

      - name: Set pending_disks
        ansible.builtin.set_fact: 
          pending_disks: "{{ ontap_rest_info['storage/disks']['records'] }}"

      - name: Log of pending_disks
        include_tasks: tasks/add_log.yml
        args: 
          apply:
            tags: ["disks"]
        vars: 
          log: 
            pending_disks: "{{ pending_disks }}"

      - name: Fail playbook if any non-healthy disks
        include_tasks: tasks/add_failure.yml
        args:
          apply:
            tags: ["disks"]
        vars: 
          failure: 
            issue: "There are broken disks and/or disks in a pending / reconstructing state."
            details: 
              broken_disks: "{{ broken_disks }}"
              pending_disks: "{{ pending_disks }}"
        when: broken_disks | length > 0 or pending_disks | length > 0 

      tags: ["disks"]

    - name: Validate deduplicated volumes
      block: 

      - name: List any deduplicated volumes over 96% 
              (`volume show -is-sis-volume true -percent-used >96 -fields aggregate,percent-used`)
        netapp.ontap.na_ontap_rest_cli: 
          <<: *login
          command: 'volume'
          params: 
            is-sis-volume: "true"
            percent-used: ">96"
            fields: "aggregate,percent-used"
          verb: "GET"
        register: verify_large_dedupe_volumes

      - name: Log of verify_large_dedupe_volumes
        include_tasks: tasks/add_log.yml
        args: 
          apply:
            tags: ["dedupe_volumes"]
        vars: 
          log: 
            large_dedupe_volumes: "{{ verify_large_dedupe_volumes }}"
          
      - block: 
        - name: If verify_large_dedupe_volumes isn't empty, gather list of aggrs involved 
          ansible.builtin.set_fact: 
            large_dedupe_volumes_aggrs: "{{ verify_large_dedupe_volumes.msg.records | map('aggregate') | list }}"
          
        - name: Log of large_dedupe_volumes_aggrs
          include_tasks: tasks/add_log.yml
          args: 
            apply:
              tags: ["dedupe_volumes"]
          vars: 
            log: 
              large_dedupe_volumes_aggrs: "{{ large_dedupe_volumes_aggrs }}"

        - name: Check aggr containing deduplicated volumes (`df -A <aggr>,<aggr2>`)
          netapp.ontap.na_ontap_ssh_command:
            <<: *login
            command: "df -A {{ large_dedupe_volumes_aggrs | join(',') }}"
            privilege: adv
            accept_unknown_host_keys: true
          register: verify_large_dedupe_volumes_aggrs

        - name: Log of verify_large_dedupe_volumes_aggrs
          include_tasks: tasks/add_log.yml
          args: 
            apply:
              tags: ["dedupe_volumes"]
          vars: 
            log: 
              verify_large_dedupe_volumes_aggrs: "{{ verify_large_dedupe_volumes_aggrs }}"

        - name: Fail playbook if any aggregates containing dedupe volumes are over 97% 
          include_tasks: tasks/add_failure.yml
          args:
            apply:
              tags: ["dedupe_volumes"]
          vars: 
            failure: 
              issue: "Aggrs containing deduplicated volumes must not exceed 97% used capacity"
              details: "{{ verify_large_dedupe_volumes_aggrs['stdout'] }}"
          when: verify_large_dedupe_volumes_aggrs['stdout'] | regex_findall('([9][7-9]|[1-9]\\d{2,})(?=\\%)', multiline=True) | length > 0

        when: verify_large_dedupe_volumes.msg.num_records > 0
      
      tags: ["dedupe_volumes"]

    - name: Check datetime 
      block: 
      - name: Make sure key is set if there are no NTP servers
        ansible.builtin.set_fact: 
          cluster_info: "{{ cluster_info | combine({'ntp_servers': []}) }}"
        when: cluster_info.ntp_servers is undefined
  
      - name: Fail playbook if there are not at least 3 NTP Servers
        include_tasks: tasks/add_failure.yml
        args:
          apply:
            tags: ["datetime"]
        vars: 
          failure: 
            issue: "There must be at least three NTP servers per NetApp best practices 
                    but there are currently {{ cluster_info.ntp_servers | length }}"
            details: "{{ cluster_info.ntp_servers }}"
        when: cluster_info.ntp_servers | length < 3

      - name: Verify cluster dates 
        ansible.builtin.set_fact: 
          cluster_dates: "{{ cluster_nodes | map(attribute='date', default='') | list }}"

      - name: Fail playbook if there is any variation among node timestamps 
        include_tasks: tasks/add_failure.yml
        args:
          apply:
            tags: ["datetime"]
        vars: 
          failure: 
            issue: "Dates and timezones must match on all nodes"
            details: "{{ cluster_dates }}"
        when: cluster_dates | unique | length > 1

      tags: ["datetime"]

    - name: Check for any offline aggregates, volumes, or SVMs
      block: 
      - name: Get any offline aggregates
        ansible.builtin.set_fact: 
          offline_aggregates: "{{ aggregate_info | selectattr('state','!=','online') | list }}"

      - name: Fail playbook for any offline aggrs 
        include_tasks: tasks/add_failure.yml
        args:
          apply:
            tags: ["offline_objects"]
        vars: 
          failure: 
            issue: "All aggregates must be online" 
            details: "{{ offline_aggregates }}"
        when: offline_aggregates | length > 0

      - name: Get any offline volumes
        ansible.builtin.set_fact: 
          offline_volumes: "{{ volume_info | selectattr('state','!=','online') | list }}"

      - name: Fail playbook for any offline volumes 
        include_tasks: tasks/add_failure.yml
        args:
          apply:
            tags: ["offline_objects"]
        vars: 
          failure: 
            issue: "All volumes must be online"
            details: "{{ offline_volumes }}"
        when: offline_volumes | length > 0

      - name: Get any offline SVMs
        ansible.builtin.set_fact: 
          offline_svms: "{{ vserver_info | selectattr('state','!=','running') | list }}"

      - name: Fail playbook for any svm's not running
        include_tasks: tasks/add_failure.yml
        args:
          apply:
            tags: ["offline_objects"]
        vars: 
          failure: 
            issue: "All SVMs must be online"
            details: "{{ offline_svms }}"
        when: offline_svms | length > 0

      tags: ["offline_objects"]

    - name: Check networking 
      block: 

      # TODO - this feels janky and I think I should be able to do this without a loop
      - name: Check for any LIFs not being home
        ansible.builtin.set_fact: 
          net_int_not_home: "{{ net_int_not_home | default([]) + [item] }}"
        loop: "{{ net_int }}"
        loop_control:
          label: "{{ item.name }}"
        when: item.location.is_home == False

      - block:
        - name: Log of LIFs not home
          include_tasks: tasks/add_log.yml
          args: 
            apply:
              tags: ["networking"]
          vars: 
            log: 
              net_int_not_home: "{{ net_int_not_home }}"

        - name: All LIFs should be homed prior to upgrades
          netapp.ontap.na_ontap_rest_cli:
            <<: *login
            command: "network/interface/revert"
            params: 
              "vserver": "*"
            verb: "PATCH"
          register: verify_net_int_revert

        - name: Log of verify_net_int_revert
          include_tasks: tasks/add_log.yml
          args: 
            apply:
              tags: ["networking"]
          vars: 
            log:  
              net_int_revert: "{{ verify_net_int_revert }}"
        
        - name: Check to make sure everything got home (net int show -is-home false)
          netapp.ontap.na_ontap_rest_cli: 
            <<: *login
            command: 'network/interface'
            params: 
              is-home: "false"
            verb: "GET"
          register: verify_all_lifs_home

        - name: Log of verify_all_lifs_home
          include_tasks: tasks/add_log.yml
          args: 
            apply:
              tags: ["networking"]
          vars: 
            log: 
              all_lifs_home: "{{ verify_all_lifs_home }}"

        - name: Fail playbook if not all LIFs are home 
          include_tasks: tasks/add_failure.yml
          args:
            apply:
              tags: ["networking"]
          vars: 
            failure: 
              issue: "All LIFs must be home"
              details: "{{ verify_all_lifs_home.msg.records }}"
          when: verify_all_lifs_home.msg.num_records > 0

        when: net_int_not_home is defined

      - name: Get ethernet ports that are UP    
        netapp.ontap.na_ontap_rest_cli: 
          <<: *login
          command: 'network/port'
          params: 
            link: "up"
            fields: "node,mtu,ifgrp-port,health-status,link,broadcast-domain,type"
          verb: "GET"
        register: verify_ports

      - name: Log of verify_ports
        include_tasks: tasks/add_log.yml
        args: 
          apply:
            tags: ["networking"]
        vars: 
          log: 
            ports: "{{ verify_ports }}"

      - name: Find any unhealthy ports and/or ports without a broadcast domain
        ansible.builtin.set_fact: 
          verify_ports_unhealthy: "{{ verify_ports.msg.records | selectattr('broadcast_domain', 'undefined') | 
                                      selectattr('health_status','!=','healthy') | list }}"

      - name: Fail playbook for any unhealthy ports and/or ports without a broadcast domain
        include_tasks: tasks/add_failure.yml
        args:
          apply:
            tags: ["networking"]
        vars: 
          failure: 
            issue: "All up link ports must be healthy and be in a broadcast domain"
            details: "{{ verify_ports_unhealthy }}"
        when: verify_ports_unhealthy | length > 0 

      - name: Get failover groups (`net int failover-groups show`)
        netapp.ontap.na_ontap_rest_cli: 
          <<: *login
          command: 'network/interface/failover-groups'
          params: 
            fields: "vserver,targets,failover-group"
          verb: "GET"
        register: verify_failover_groups
      
      - name: Log of verify_failover_groups
        include_tasks: tasks/add_log.yml
        args: 
          apply:
            tags: ["networking"]
        vars: 
          log: 
            failover_groups: "{{ verify_failover_groups }}"

      - name: Find any invalid failover groups domains
        ansible.builtin.set_fact: 
          invalid_failover_groups: "{{ verify_failover_groups.msg.records | ontap_find_invalid_failover_groups(cluster_nodes) }}"

      - name: Fail playbook if, for each failover group, a) There isn't at least 1 port from each node 
                b) The vlan tags do not match
        include_tasks: tasks/add_failure.yml
        args:
          apply:
            tags: ["networking"]
        vars: 
          failure: 
            issue: "Each failover group must have at least 1 port from each node ({{ cluster_nodes | map(attribute='name') | join(',') }}) and the vlan tags must 
                      match"
            details: "{{ invalid_failover_groups }}"
        when: invalid_failover_groups | length > 0

      - name: Get broadcast domains (`broadcast-domain show`)  
        netapp.ontap.na_ontap_rest_cli: 
          <<: *login
          command: 'network/port/broadcast-domain'
          params: 
            fields: "broadcast-domain,ports,mtu,ipspace,failover-groups"
          verb: "GET"
        register: verify_broadcast_domains

      - name: Log of verify_broadcast_domains
        include_tasks: tasks/add_log.yml
        args: 
          apply:
            tags: ["networking"]
        vars: 
          log: 
            broadcast_domains: "{{ verify_broadcast_domains }}"

      - name: Find any invalid broadcast domains
        ansible.builtin.set_fact: 
          invalid_broadcast_domains: "{{ verify_broadcast_domains.msg.records | ontap_find_invalid_broadcast_domains(cluster_nodes) }}"

      - name: Fail playbook if, for each broadcast domain, a) There isn't at least 1 port from each node 
                b) The vlan tags do not match
        include_tasks: tasks/add_failure.yml
        args:
          apply:
            tags: ["networking"]
        vars: 
          failure: 
            issue: "Each broadcast domain must have at least 1 port from each node ({{ cluster_nodes | map(attribute='name') | join(',') }}) and the vlan tags must 
                      match"
            details: "{{ invalid_broadcast_domains }}"
        when: invalid_broadcast_domains | length > 0
      
      tags: ["networking"]  

    - name: Check what jobs are running 
      block:
      - name: Get running jobs 
        netapp.ontap.na_ontap_rest_cli: 
          <<: *login
          command: 'job'
          params: 
            fields: "name,id,node,state,vserver"
            state: "running"  
          verb: "GET"
        register: verify_running_jobs
      
      - name: Log of verify_running_jobs
        include_tasks: tasks/add_log.yml
        args: 
          apply:
            tags: ["jobs"]
        vars: 
          log: 
            running_jobs: "{{ verify_running_jobs }}"

      - name: Fail playbook if any jobs are running besides {{ allowed_jobs | join(',') }}
        include_tasks: tasks/add_failure.yml  
        args:
          apply:
            tags: ["jobs"]
        vars: 
          failure: 
            issue: "No jobs should be running"
            details: "{{ verify_running_jobs.msg.records }}"
        when: verify_running_jobs.msg.num_records > 0 
                and 
              verify_running_jobs.msg.records | map(attribute='name') | difference(allowed_jobs) | length > 0

      tags: ["jobs"]

    - block: 
      - name: Ensure HA is enabled for two node clusters
        netapp.ontap.na_ontap_cluster_ha:
          <<: *login
          state: present
        register: enable_ha

      - name: Log of enable_ha
        include_tasks: tasks/add_log.yml
        args: 
          apply:
            tags: ["jobs"]
        vars: 
          log: 
            enable_ha: "{{ enable_ha }}"
      when: cluster_node_count | int == 2 

    - name: Summary of health checks 
      block: 

      - name: Get uptime (`system node show -fields uptime`) 
        netapp.ontap.na_ontap_rest_cli: 
          <<: *login
          command: 'system/node'
          params: 
            fields: "model,uptime"
          verb: "GET"
        register: verify_uptime

      - name: Get current ONTAP Version
        netapp.ontap.na_ontap_rest_cli: 
          <<: *login
          command: 'system/image'
          params: 
            iscurrent: "true"
            fields: "image,node,version"
          verb: "GET"
        register: verify_ontap_version

      - name: Show uptime and current ONTAP Version
        include_tasks: tasks/add_log.yml
        vars: 
          log: 
            uptime_and_os: "{{ verify_ontap_version.msg.records | community.general.lists_mergeby(verify_uptime.msg.records, 'node') }}"

      - name: Show summary_logs
        ansible.builtin.debug: 
          var: summary_logs 
          
      - name: Show summary_warnings
        ansible.builtin.fail: 
          msg: "See Above Warnings"
        when: summary_warnings | warning | length > 0 
        ignore_errors: True

      - name: Fail playbook if summary_failures
        ansible.builtin.fail: 
          msg: "{{ summary_failures }}"
        when: summary_failures | length > 0
      
      tags:
        - always


