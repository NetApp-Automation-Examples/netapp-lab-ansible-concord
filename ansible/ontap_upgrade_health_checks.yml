---
- name: "Run Upgrade Health Checks on {{ clusters }}"
  hosts: "{{ clusters | default('localhost') }}"
  gather_facts: no
  connection: 'local'
  collections:
    - netapp.ontap
  module_defaults:
    group/netapp.ontap.netapp_ontap:
      hostname: "{{ ontap_hostname }}"
      username: "{{ ontap_username }}"
      password: "{{ ontap_password }}"
      https: "{{ https }}"
      validate_certs: "{{ validate_certs }}"
  vars:
    summary_failures: []
    summary_warnings: []
    summary_logs: []
    allowed_jobs: ["FabricPool Space Job", "SP Certificate Expiry Check Job"]
    cluster_rings: ["vldb","mgmt","vifmgr","bcomd","crs"]
    check_for_not_online: ["storage/aggregates", "storage/volumes", "svm/svms"]
    post_upgrade_check: False
    pre_or_post_task_name: Run {{ 'post' if post_upgrade_check else 'pre' }}-upgrade health checks 
    upgrade_status: []
  pre_tasks: 
    - name: "{{ pre_or_post_task_name }}"
      ansible.builtin.debug:
        var: post_upgrade_check
    - name: "Upgrade Status"
      ansible.builtin.debug:
        var: upgrade_status
  tasks:

    - name: Get cluster info
      block: 
      - name: Get cluster info (`netapp.ontap.na_ontap_rest_info`)
        include_tasks: tasks/rest_info.yml
        vars: 
          gather_subset:
            - cluster
            - cluster/nodes
          fields: 
            - "*"

      - name: Set needed variables
        ansible.builtin.set_fact: 
          cluster_info: "{{ ontap_rest_info['cluster'] }}"
          cluster_node_count: "{{ ontap_rest_info['cluster/nodes']['num_records'] }}"
          cluster_nodes: "{{ ontap_rest_info['cluster/nodes']['records'] }}"

      tags: ["always"]

    - name: Check overall health of the cluster
      block: 

      - include_tasks: tasks/health_checks/health_alerts.yml

      - include_tasks: tasks/health_checks/cluster_health.yml

      - include_tasks: tasks/health_checks/failover.yml

      tags: ["health"]

    - name: Check system utilization  
      block: 

      - include_tasks: tasks/health_checks/cpu_disk_utilization.yml    
      
      tags: ["cpu_disk"]

    - name: Check cluster ring database epoch and quorum master
      block: 

      - include_tasks: tasks/health_checks/cluster_rings.yml    

      tags: ["cluster_rings"]

    # Not sure this is neccessary     
    # - name: Make sure cluster epsilon is at last node 
    #   block: 

    #   - include_tasks: tasks/health_checks/move_epsilon.yml    
      
    #   when: cluster_node_count | int > 2
    #   tags: ["epsilon"]
    
    - name: Check SAN configuration
      block: 

      - include_tasks: tasks/health_checks/san.yml    

      tags: ["san"] 

    - name: Check SnapMirrors and SnapShot count
      block: 

      - include_tasks: tasks/health_checks/dp_snapmirror.yml
      
      - name: Warn about DP SnapMirror upgrade order
        include_tasks: tasks/add_warning.yml
        vars: 
          warning: 
            issue: "**Destination cluster must be upgraded prior 
                    to upgrading source cluster for DP SnampMirrors** 
                    See https://docs.netapp.com/us-en/ontap/upgrade/task_preparing_snapmirror_relationships_for_a_nondisruptive_upgrade_or_downgrade.html"
            details: "{{ verify_snapmirror_dp_destinations.msg.records | dicts_to_table }}"
        when: verify_snapmirror_dp_destinations.msg.num_records > 0

      tags: ["dp_snapmirror"]

    - name: Check snapshot count
      block: 

      - include_tasks: tasks/health_checks/snapshots_count.yml

      - name: Fail playbook if there are more than 20,000 snapshot copies
        include_tasks: tasks/add_failure.yml
        vars: 
          failure: 
            issue: "Each node must not exceed 20,000 snapshot copies"
            details: "{{ verify_snapshot_count.msg.num_records }} snapshot copies"
        when: verify_snapshot_count.msg.num_records > 20000

      tags: ["snapshot_count"]

    - name: Check for bad disks
      block:

      - include_tasks: tasks/health_checks/disks.yml

      tags: ["disks"]

    - name: Check for inconsistent volumes and validate deduplicated volumes 
      block: 
      
      - include_tasks: tasks/health_checks/inconsistent_volumes.yml

      - include_tasks: tasks/health_checks/dedupe_volumes.yml      

      tags: ["volumes"]
            
    - name: Check date and time 
      block: 

      - include_tasks: tasks/health_checks/date_time.yml      

      tags: ["datetime"]

    - name: Check for any offline objects
      block: 
      - include_tasks: tasks/health_checks/not_online.yml    
        vars: 
          online_object_check: "{{ item }}"
        loop: "{{ check_for_not_online }}" 
      tags: ["not_online"]

    - name: Check networking 
      block: 

      - include_tasks: tasks/health_checks/networking.yml    
     
      tags: ["networking"]  

    - name: Check what jobs are running (pre-upgrade only)
      block:


      - include_tasks: tasks/health_checks/running_jobs.yml    

      - name: Fail playbook if any jobs are running besides {{ allowed_jobs | join(',') }}
        include_tasks: tasks/add_failure.yml  
        vars: 
          failure: 
            issue: "No jobs should be running"
            details: "{{ verify_running_jobs.msg.records | dicts_to_table }}"
        when: verify_running_jobs.msg.num_records > 0 
                and 
              verify_running_jobs.msg.records | map(attribute='name') | difference(allowed_jobs) | length > 0

      tags: ["jobs"]
      when: post_upgrade_check != True

    - name: Summary of health checks 
      block: 

      - include_tasks: tasks/health_checks/ensure_ha.yml    
        when: cluster_node_count | int == 2 

      - name: Show HA (`cluster ha show`) 
        netapp.ontap.na_ontap_rest_cli: 
          command: 'cluster/ha'
          verb: "GET"
        register: verify_ha

      - name: Get uptime (`system node show -fields uptime`) 
        netapp.ontap.na_ontap_rest_cli: 
          command: 'system/node'
          params: 
            fields: "model,uptime"
          verb: "GET"
        register: verify_uptime

      - name: Get current ONTAP Version (`system image show -iscurrent true -fields image,node,version`)
        netapp.ontap.na_ontap_rest_cli: 
          command: 'system/image'
          params: 
            iscurrent: "true"
            fields: "image,node,version"
          verb: "GET"
        register: verify_ontap_version

      - name: Set uptime_and_os
        ansible.builtin.set_fact: 
          uptime_and_os: "{{ verify_ontap_version.msg.records 
                            | community.general.lists_mergeby(verify_uptime.msg.records, 'node') }}"

      - name: Show uptime and current ONTAP Version
        include_tasks: tasks/add_log.yml
        vars: 
          log: 
            cluster ha show: "{{ verify_ha.msg }}"
            Current Uptime and OS: "{{ uptime_and_os | dicts_to_table }}"
      
      - name: Show summary_logs
        ansible.builtin.debug: 
          var: summary_logs
          
      - name: Show summary_warnings
        ansible.builtin.fail: 
          msg: "See Above Warnings"
        when: summary_warnings | warning | length > 0 
        ignore_errors: True

      - name: Fail playbook if summary_failures
        ansible.builtin.fail: 
          msg: "{{ summary_failures }}"
        when: summary_failures | length > 0
      
      tags:
        - always


